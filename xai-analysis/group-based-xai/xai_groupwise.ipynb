{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import os\n",
    "import os.path \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import copy\n",
    "import errno\n",
    "import glob\n",
    "import time\n",
    "import calendar\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import netCDF4\n",
    "from numpy import savez_compressed\n",
    "from optparse import OptionParser\n",
    "from scipy.interpolate import (UnivariateSpline, RectBivariateSpline, RegularGridInterpolator)\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data1/fog/Hamid/FogNet/') \n",
    "import src\n",
    "from src import utils, FogNet, FogNetConfig, cnn_evaluate, xai_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Number of devices: 4\n",
      "NAM_G1 input size:  (5460, 32, 32, 108, 1)\n",
      "NAM_G2 input size:  (5460, 32, 32, 96, 1)\n",
      "NAM_G3 input size:  (5460, 32, 32, 108, 1)\n",
      "NAM_G4 input size:  (5460, 32, 32, 60, 1)\n",
      "MIXED input size:  (5460, 32, 32, 12, 1)\n",
      "MUR input size:  (5460, 384, 384, 1, 1)\n",
      "NAM_G1 input size:  (3328, 32, 32, 108, 1)\n",
      "NAM_G2 input size:  (3328, 32, 32, 96, 1)\n",
      "NAM_G3 input size:  (3328, 32, 32, 108, 1)\n",
      "NAM_G4 input size:  (3328, 32, 32, 60, 1)\n",
      "MIXED input size:  (3328, 32, 32, 12, 1)\n",
      "MUR input size:  (3328, 384, 384, 1, 1)\n",
      "NAM_G1 input size:  (2228, 32, 32, 108, 1)\n",
      "NAM_G2 input size:  (2228, 32, 32, 96, 1)\n",
      "NAM_G3 input size:  (2228, 32, 32, 108, 1)\n",
      "NAM_G4 input size:  (2228, 32, 32, 60, 1)\n",
      "MIXED input size:  (2228, 32, 32, 12, 1)\n",
      "MUR input size:  (2228, 384, 384, 1, 1)\n",
      "training target shape: (5460,)\n",
      "training categorical target shape: (5460, 2)\n",
      "validation target shape: (3328,)\n",
      "validation categorical target shape: (3328, 2)\n",
      "testing target shape: (2228,)\n",
      "testing categorical target shape: (2228, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import os\n",
    "import os.path \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import copy\n",
    "import errno\n",
    "import glob\n",
    "import time\n",
    "import calendar\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import netCDF4\n",
    "import copy\n",
    "from numpy import savez_compressed\n",
    "from optparse import OptionParser\n",
    "from scipy.interpolate import (UnivariateSpline, RectBivariateSpline, RegularGridInterpolator)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import statistics\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model   \n",
    "from tensorflow.keras.layers import Add, add, concatenate, Reshape, BatchNormalization, Input, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, Conv3D, Activation, MaxPooling2D, MaxPooling3D, AveragePooling3D, ReLU, GlobalAveragePooling3D, multiply\n",
    " \n",
    "from tensorflow.keras import regularizers \n",
    "from tensorflow.keras import optimizers \n",
    "from tensorflow.keras.optimizers import Adam, SGD \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "DEFAULT_IMAGE_DIR_NAME = ('/data1/fog-data/fog-maps/')\n",
    "DEFAULT_TARGET_DIR_NAME = ('/data1/fog/fognn/Dataset/TARGET/')\n",
    "\n",
    "DEFAULT_CUBES_24_DIR_NAME = ('/data1/fog/fognn/Dataset/24HOURS/INPUT/')\n",
    "DEFAULT_TARGET_24_DIR_NAME = ('/data1/fog/fognn/Dataset/24HOURS/TARGET/')\n",
    "\n",
    "DEFAULT_LINE_COLOUR = numpy.array([228, 26, 28], dtype=float) / 255\n",
    "DEFAULT_LINE_WIDTH = 3\n",
    "DEFAULT_RANDOM_LINE_COLOUR = numpy.full(3, 152. / 255)\n",
    "DEFAULT_RANDOM_LINE_WIDTH = 2\n",
    "\n",
    "LEVELS_FOR_CONTOURS = numpy.linspace(0, 1, num=11, dtype=float)\n",
    "\n",
    "FIGURE_WIDTH_INCHES = 10\n",
    "FIGURE_HEIGHT_INCHES = 10\n",
    "\n",
    "FONT_SIZE = 20\n",
    "plt.rc('font', size=FONT_SIZE)\n",
    "plt.rc('axes', titlesize=FONT_SIZE)\n",
    "plt.rc('axes', labelsize=FONT_SIZE)\n",
    "plt.rc('xtick', labelsize=FONT_SIZE)\n",
    "plt.rc('ytick', labelsize=FONT_SIZE)\n",
    "plt.rc('legend', fontsize=FONT_SIZE)\n",
    "plt.rc('figure', titlesize=FONT_SIZE)\n",
    "\n",
    "strategy = tensorflow.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "############################\n",
    "# Setup input data rasters #\n",
    "############################\n",
    "# Generate data file paths\n",
    "trainYearIdxs = [4, 5, 6, 7, 8]\n",
    "valYearIdxs   = [0, 1, 2, 3]\n",
    "testYearIdxs  = [9, 10, 11]\n",
    "\n",
    "horizons = [6, 12, 24]\n",
    "allYears = [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "#strategy = tensorflow.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "#print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "#with strategy.scope(): \n",
    "    \n",
    "nam_G1_template = \"NETCDF_NAM_CUBE_{year}_PhG1_{horizon}.npz\"\n",
    "nam_G1_names = [nam_G1_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "nam_G2_template = \"NETCDF_NAM_CUBE_{year}_PhG2_{horizon}.npz\"\n",
    "nam_G2_names = [nam_G2_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "nam_G3_template = \"NETCDF_NAM_CUBE_{year}_PhG3_{horizon}.npz\"\n",
    "nam_G3_names = [nam_G3_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "nam_G4_template = \"NETCDF_NAM_CUBE_{year}_PhG4_{horizon}.npz\"\n",
    "nam_G4_names = [nam_G4_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "mixed_file_template = \"NETCDF_MIXED_CUBE_{year}_{horizon}.npz\"\n",
    "mixed_file_names = [mixed_file_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "mur_file_template = \"NETCDF_SST_CUBE_{year}.npz\"\n",
    "mur_file_names = [mur_file_template.format(year=year) for year in allYears]\n",
    "\n",
    "targets_file_template = \"target{year}_{horizon}.csv\"\n",
    "targets_file_names = [targets_file_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "\n",
    "    # Read data cubes\n",
    "training_list   = utils.load_Cat_cube_data(nam_G1_names,\n",
    "    nam_G2_names, nam_G3_names, nam_G4_names, mixed_file_names, mur_file_names, DEFAULT_CUBES_24_DIR_NAME, trainYearIdxs)\n",
    "\n",
    "validation_list = utils.load_Cat_cube_data(nam_G1_names,\n",
    "    nam_G2_names, nam_G3_names, nam_G4_names, mixed_file_names, mur_file_names, DEFAULT_CUBES_24_DIR_NAME, valYearIdxs)\n",
    "\n",
    "testing_list    = utils.load_Cat_cube_data(nam_G1_names,\n",
    "    nam_G2_names, nam_G3_names, nam_G4_names, mixed_file_names, mur_file_names, DEFAULT_CUBES_24_DIR_NAME, testYearIdxs)\n",
    "\n",
    "target_class = utils.targets(\n",
    "    targets_file_names, trainYearIdxs, valYearIdxs, testYearIdxs,\n",
    "    DEFAULT_TARGET_24_DIR_NAME,\n",
    "    0, # priority_calss: the last integer value is the class of target to predict: 0: is < 1600; 1: < 3200 and 2: < 6400\n",
    ")\n",
    "target_list = target_class.binary_target()\n",
    "\n",
    "# Separate into train, test, validation\n",
    "Training_targets = target_list[0]\n",
    "print('training target shape:', Training_targets.shape)\n",
    "ytrain = target_list[1]\n",
    "print('training categorical target shape:', ytrain.shape)\n",
    "Validation_targets = target_list[2]\n",
    "print('validation target shape:', Validation_targets.shape)\n",
    "yvalid = target_list[3]\n",
    "print('validation categorical target shape:', yvalid.shape)\n",
    "Testing_targets = target_list[4]\n",
    "print('testing target shape:', Testing_targets.shape)\n",
    "ytest = target_list[5]\n",
    "print('testing categorical target shape:', ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#with strategy.scope():\n",
    "    # Initialize\n",
    "learningRate = 0.0009 # hyperparameters[key][0] \n",
    "wd           = 0.001  # hyperparameters[key][1] \n",
    "filters      = 24     # hyperparameters[key][2] \n",
    "dropout      = 0.3    # hyperparameters[key][3] \n",
    "\n",
    "\n",
    "cnn_file_name = '/data1/fog/FogNet/trained_model/single_gpu_weights.h5'\n",
    "#cnn_file_name = '/data1/fog/fognn/FogNet/trained_model/weights.h5'\n",
    "input_nam_shape       = Input((32, 32, 288, 1))\n",
    "input_mur_shape       = Input((384, 384, 1, 1)) \n",
    "input_nam_G1_24_shape = Input((32, 32, 108, 1))\n",
    "input_nam_G2_24_shape = Input((32, 32, 96, 1))\n",
    "input_nam_G3_24_shape = Input((32, 32, 108, 1))\n",
    "input_nam_G4_24_shape = Input((32, 32, 60, 1))\n",
    "input_mixed_24_shape  = Input((32, 32, 12, 1))\n",
    "\n",
    "\n",
    "C  = FogNet.FogNet(input_nam_G1_24_shape, \n",
    "               input_nam_G2_24_shape, \n",
    "               input_nam_G3_24_shape, \n",
    "               input_nam_G4_24_shape, \n",
    "               input_mixed_24_shape, \n",
    "               input_mur_shape, filters, dropout, 2)\n",
    "\n",
    "cnn_model_object = C.BuildModel()\n",
    "#cnn_model_object = multi_gpu_model(cnn_model_object, gpus=4)\n",
    "#model.summary() \n",
    "\n",
    "cnn_model_object.load_weights(cnn_file_name)  \n",
    "\n",
    "cnn_model_object.compile(optimizer=Adam(lr=learningRate, decay=wd),\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_group_names(gnames):\n",
    "    out = []\n",
    "    times = ['T1', 'T2', 'T3', 'T4']\n",
    "    for name in gnames: \n",
    "        for t in times: \n",
    "            out.append(name +'_' + t)\n",
    "            \n",
    "    return out \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3')\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3'), communication = CommunicationImplementation.AUTO\n",
      "analysed_sst_T1| PSS = 0.54, 0.00| HSS = 0.52, 0.00| CSS = 0.51, 0.00\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "mean_hss, mean_pss, mean_css = [], [], []\n",
    "std_hss, std_pss, std_css = [], [], []\n",
    "fnames, gnames = [], []\n",
    "\n",
    "#n_groups   = len(input_data)\n",
    "\n",
    "g = 5\n",
    "\n",
    "if g ==0:\n",
    "    GNames   = utils.NETCDF_PREDICTOR_NAMES['Physical_G1']\n",
    "elif g ==1:\n",
    "    GNames = utils.NETCDF_PREDICTOR_NAMES['Physical_G2']\n",
    "elif g ==2:\n",
    "    GNames = utils.NETCDF_PREDICTOR_NAMES['Physical_G3']\n",
    "elif g ==3:\n",
    "    GNames = utils.NETCDF_PREDICTOR_NAMES['Physical_G4']\n",
    "elif g ==4:\n",
    "    GNames = utils.NETCDF_MIXED_NAMES\n",
    "elif g ==5:\n",
    "    GNames = utils.NETCDF_MUR_NAMES  \n",
    "\n",
    "GNames_t = copy_group_names(GNames)\n",
    "n_features = testing_list[g].shape[3]\n",
    "\n",
    "\n",
    "random_state=42\n",
    "strategy = tensorflow.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "#print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "with strategy.scope(): \n",
    "    for f in range(0, 4):\n",
    "\n",
    "        this_hss, this_pss, this_css = [], [], []\n",
    "\n",
    "        #for i in range(2):\n",
    "            #print(f\"Iteration {i}: group {g}, feature name {GNames[f]}!\")\n",
    "\n",
    "        input_data_copy    = copy.deepcopy(testing_list)\n",
    "        numpy.random.seed(random_state)\n",
    "        permuted_map       = numpy.random.permutation(input_data_copy[g][:,:,:,f,:]) \n",
    "        input_data_copy[g][:,:,:,f,:] = permuted_map\n",
    "\n",
    "        y_testing_cat_prob = cnn_model_object.predict(input_data_copy, batch_size = 4) \n",
    "        metric_list        = cnn_evaluate.test_eval(Testing_targets, y_testing_cat_prob, threshold = 0.193)\n",
    "\n",
    "        this_pss.append(metric_list[8])\n",
    "        this_hss.append(metric_list[9])\n",
    "        this_css.append(metric_list[11])\n",
    "\n",
    "        feature_name   = GNames_t[f]\n",
    "        fnames.append(feature_name)\n",
    "        gnames.append(g)\n",
    "\n",
    "        mean_pss.append(statistics.mean(this_pss))\n",
    "        std_pss.append(statistics.pstdev(this_pss))\n",
    "\n",
    "        mean_hss.append(statistics.mean(this_hss))\n",
    "        std_hss.append(statistics.pstdev(this_hss))\n",
    "\n",
    "        mean_css.append(statistics.mean(this_css))\n",
    "        std_css.append(statistics.pstdev(this_css)) \n",
    "        \n",
    "        print(f\"{GNames_t[f]}| PSS = {statistics.mean(this_pss):.2f}, {statistics.pstdev(this_pss):.2f}| HSS = {statistics.mean(this_hss):.2f}, {statistics.pstdev(this_hss):.2f}| CSS = {statistics.mean(this_css):.2f}, {statistics.pstdev(this_css):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GNames = utils.NETCDF_PREDICTOR_NAMES['Physical_G4']\n",
    "len(GNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FogNet (24 Hr. 1600m) based on PSS, HSS, and CSS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_targets  = pd.read_csv(os.path.join('/data1/fog/Hamid/FogNet/trained_model/VIS_True_TRAIN.csv')) \n",
    "Training_targets  = Training_targets[['CAT1600']].to_numpy().ravel()\n",
    "Testing_targets      = pd.read_csv(os.path.join('/data1/fog/Hamid/FogNet/trained_model/VIS_True_TEST.csv'))\n",
    "Testing_targets      = Testing_targets[['CAT1600']].to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fognet_training_cat_prob  = pd.read_csv(os.path.join('/data1/fog/Hamid/FogNet/trained_model/VIS_Prob_TRAIN.csv'))\n",
    "fognet_training_cat_prob  = fognet_training_cat_prob[['C0_Prob', 'C1_Prob']].to_numpy()\n",
    "fognet_test_cat_prob      = pd.read_csv(os.path.join('/data1/fog/Hamid/FogNet/trained_model/VIS_Prob_TEST.csv'))\n",
    "fognet_test_cat_prob      = fognet_test_cat_prob[['C0_Prob', 'C1_Prob']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hss_training_threshold, hss_accuray_list, hss_Res241600  = cnn_evaluate.skilled_metrics(Training_targets, fognet_training_cat_prob, 'HSS')\n",
    "hss_testing_accuracy                      = cnn_evaluate.test_eval(Testing_targets, fognet_test_cat_prob, hss_training_threshold)\n",
    "pss_training_threshold, pss_accuray_list, pss_Res241600  = cnn_evaluate.skilled_metrics(Training_targets, fognet_training_cat_prob, 'PSS')\n",
    "pss_testing_accuracy                      = cnn_evaluate.test_eval(Testing_targets, fognet_test_cat_prob, pss_training_threshold)\n",
    "css_training_threshold, css_accuray_list, css_Res241600  = cnn_evaluate.skilled_metrics(Training_targets, fognet_training_cat_prob, 'CSS')\n",
    "css_testing_accuracy                      = cnn_evaluate.test_eval(Testing_targets, fognet_test_cat_prob, css_training_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  HSS  ------  PSS  ------  CSS\n",
      "POD:   0.55         0.58         0.28\n",
      "F:     0.02         0.03         0.00\n",
      "FAR:   0.49         0.61         0.32\n",
      "CSI:   0.36         0.31         0.25\n",
      "PSS:   0.54         0.55         0.28\n",
      "HSS:   0.52         0.45         0.39\n",
      "ORSS:  0.97         0.96         0.98\n",
      "CSS:   0.50         0.38         0.66\n"
     ]
    }
   ],
   "source": [
    "print(f\"------  HSS  ------  PSS  ------  CSS\")\n",
    "print(f\"POD:   {hss_testing_accuracy[4]:.2f}         {pss_testing_accuracy[4]:.2f}         {css_testing_accuracy[4]:.2f}\")\n",
    "print(f\"F:     {hss_testing_accuracy[5]:.2f}         {pss_testing_accuracy[5]:.2f}         {css_testing_accuracy[5]:.2f}\")\n",
    "print(f\"FAR:   {hss_testing_accuracy[6]:.2f}         {pss_testing_accuracy[6]:.2f}         {css_testing_accuracy[6]:.2f}\")\n",
    "print(f\"CSI:   {hss_testing_accuracy[7]:.2f}         {pss_testing_accuracy[7]:.2f}         {css_testing_accuracy[7]:.2f}\")\n",
    "print(f\"PSS:   {hss_testing_accuracy[8]:.2f}         {pss_testing_accuracy[8]:.2f}         {css_testing_accuracy[8]:.2f}\")\n",
    "print(f\"HSS:   {hss_testing_accuracy[9]:.2f}         {pss_testing_accuracy[9]:.2f}         {css_testing_accuracy[9]:.2f}\")\n",
    "print(f\"ORSS:  {hss_testing_accuracy[10]:.2f}         {pss_testing_accuracy[10]:.2f}         {css_testing_accuracy[10]:.2f}\")\n",
    "print(f\"CSS:   {hss_testing_accuracy[11]:.2f}         {pss_testing_accuracy[11]:.2f}         {css_testing_accuracy[11]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on HSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation HSS based results:\n",
      "------  Score\n",
      "G1:    1.46\n",
      "G2:    -0.14\n",
      "G3:    0.67\n",
      "G4:    -0.98\n",
      "G5:    2.23\n"
     ]
    }
   ],
   "source": [
    "pfi_df = pd.read_csv('./PFIGW.csv')\n",
    "permut_hss_results = pfi_df['HSS_Mean']\n",
    "\n",
    "permutation_hss_results = [(0.52 - permut_hss_results[i]) for i in range(5)]\n",
    "print(\"Permutation HSS based results:\")\n",
    "print(\"------  Score\")\n",
    "print(f\"G1:    {permutation_hss_results[0] * 100:.2f}\")\n",
    "print(f\"G2:    {permutation_hss_results[1] * 100:.2f}\")\n",
    "print(f\"G3:    {permutation_hss_results[2] * 100:.2f}\")\n",
    "print(f\"G4:    {permutation_hss_results[3] * 100:.2f}\")\n",
    "print(f\"G5:    {permutation_hss_results[4] * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on PSS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation PSS based results:\n",
      "------  Score\n",
      "G1:    1.53\n",
      "G2:    1.35\n",
      "G3:    1.44\n",
      "G4:    1.26\n",
      "G5:    1.63\n"
     ]
    }
   ],
   "source": [
    "pfi_df = pd.read_csv('./PFIGW.csv')\n",
    "permut_pss_results = pfi_df['PSS_Mean']\n",
    "\n",
    "permutation_pss_results = [(0.55 - permut_pss_results[i]) for i in range(5)]\n",
    "print(\"Permutation PSS based results:\")\n",
    "print(\"------  Score\")\n",
    "print(f\"G1:    {permutation_pss_results[0] * 100:.2f}\")\n",
    "print(f\"G2:    {permutation_pss_results[1] * 100:.2f}\")\n",
    "print(f\"G3:    {permutation_pss_results[2] * 100:.2f}\")\n",
    "print(f\"G4:    {permutation_pss_results[3] * 100:.2f}\")\n",
    "print(f\"G5:    {permutation_pss_results[4] * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Hold Hout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehout_dir = '/data1/fog/Hamid/FogNet/trained_model/xai_trained_models/onehout/'\n",
    "onehout_folder_list = sorted(os.listdir(onehout_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximize Based on HSS and return HSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehout_hss_results, onehout_pss_results = [], []\n",
    "\n",
    "for i in range(len(onehout_folder_list)):\n",
    "    y_training_cat_prob  = pd.read_csv(os.path.join(onehout_dir + onehout_folder_list[i] + '/VIS_Prob_TRAIN.csv'))\n",
    "    y_training_cat_prob  = y_training_cat_prob[['C0_Prob', 'C1_Prob']].to_numpy()\n",
    "    y_test_cat_prob      = pd.read_csv(os.path.join(onehout_dir + onehout_folder_list[i] + '/VIS_Prob_TEST.csv'))\n",
    "    y_test_cat_prob      = y_test_cat_prob[['C0_Prob', 'C1_Prob']].to_numpy()\n",
    "    \n",
    "    training_threshold_hss, _, _   = cnn_evaluate.skilled_metrics(Training_targets, y_training_cat_prob, 'HSS')\n",
    "    training_threshold_pss, _, _   = cnn_evaluate.skilled_metrics(Training_targets, y_training_cat_prob, 'PSS')\n",
    "    \n",
    "    testing_accuracy_basedon_hss =  cnn_evaluate.test_eval(Testing_targets, y_test_cat_prob, training_threshold_hss)\n",
    "    testing_accuracy_basedon_pss =  cnn_evaluate.test_eval(Testing_targets, y_test_cat_prob, training_threshold_pss)\n",
    "\n",
    "\n",
    "    onehout_hss_results.append(testing_accuracy_basedon_hss) \n",
    "    onehout_pss_results.append(testing_accuracy_basedon_pss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores based on HSS and Maximize based on HSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneGHoldout HSS based on HSS maximized results:\n",
      "------ Score\n",
      "G1:    21.44\n",
      "G2:    37.32\n",
      "G3:    42.10\n",
      "G4:    41.70\n",
      "G5:    35.58\n"
     ]
    }
   ],
   "source": [
    "onegout_only_hss_basedon_hss = [(0.52 - onehout_hss_results[i][9]) for i in range(5)]\n",
    "print(\"OneGHoldout HSS based on HSS maximized results:\")\n",
    "print(\"------ Score\")\n",
    "print(f\"G1:    {onegout_only_hss_basedon_hss[0] * 100:.2f}\")\n",
    "print(f\"G2:    {onegout_only_hss_basedon_hss[1] * 100:.2f}\")\n",
    "print(f\"G3:    {onegout_only_hss_basedon_hss[2] * 100:.2f}\")\n",
    "print(f\"G4:    {onegout_only_hss_basedon_hss[3] * 100:.2f}\")\n",
    "print(f\"G5:    {onegout_only_hss_basedon_hss[4] * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores based on PSS and Maximize based on HSS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneGHoldout PSS based on HSS maximized results:\n",
      "------ Score\n",
      "G1:    26.55\n",
      "G2:    23.39\n",
      "G3:    34.08\n",
      "G4:    -0.31\n",
      "G5:    6.71\n"
     ]
    }
   ],
   "source": [
    "onegout_only_pss_basedon_hss = [(0.54 - onehout_hss_results[i][11]) for i in range(5)]\n",
    "print(\"OneGHoldout PSS based on HSS maximized results:\")\n",
    "print(\"------ Score\")\n",
    "print(f\"G1:    {onegout_only_pss_basedon_hss[0] * 100:.2f}\")\n",
    "print(f\"G2:    {onegout_only_pss_basedon_hss[1] * 100:.2f}\")\n",
    "print(f\"G3:    {onegout_only_pss_basedon_hss[2] * 100:.2f}\")\n",
    "print(f\"G4:    {onegout_only_pss_basedon_hss[3] * 100:.2f}\")\n",
    "print(f\"G5:    {onegout_only_pss_basedon_hss[4] * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores based on PSS and Maximize based on PSS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneGHoldout PSS based on PSS maximized results:\n",
      "------ Score\n",
      "G1:    30.39\n",
      "G2:    39.95\n",
      "G3:    35.08\n",
      "G4:    25.83\n",
      "G5:    22.68\n"
     ]
    }
   ],
   "source": [
    "onegout_only_pss_basedon_pss = [(0.554324628592346 - onehout_pss_results[i][11]) for i in range(5)]\n",
    "print(\"OneGHoldout PSS based on PSS maximized results:\")\n",
    "print(\"------ Score\")\n",
    "print(f\"G1:    {onegout_only_pss_basedon_pss[0] * 100:.2f}\")\n",
    "print(f\"G2:    {onegout_only_pss_basedon_pss[1] * 100:.2f}\")\n",
    "print(f\"G3:    {onegout_only_pss_basedon_pss[2] * 100:.2f}\")\n",
    "print(f\"G4:    {onegout_only_pss_basedon_pss[3] * 100:.2f}\")\n",
    "print(f\"G5:    {onegout_only_pss_basedon_pss[4] * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores based on HSS and Maximize based on PSS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneGHoldout HSS based on PSS maximized results:\n",
      "------ Score\n",
      "G1:    16.02\n",
      "G2:    23.96\n",
      "G3:    35.10\n",
      "G4:    13.88\n",
      "G5:    14.95\n"
     ]
    }
   ],
   "source": [
    "onegout_only_hss_basedon_pss = [(0.45 - onehout_pss_results[i][9]) for i in range(5)]\n",
    "print(\"OneGHoldout HSS based on PSS maximized results:\")\n",
    "print(\"------ Score\")\n",
    "print(f\"G1:    {onegout_only_hss_basedon_pss[0] * 100:.2f}\")\n",
    "print(f\"G2:    {onegout_only_hss_basedon_pss[1] * 100:.2f}\")\n",
    "print(f\"G3:    {onegout_only_hss_basedon_pss[2] * 100:.2f}\")\n",
    "print(f\"G4:    {onegout_only_hss_basedon_pss[3] * 100:.2f}\")\n",
    "print(f\"G5:    {onegout_only_hss_basedon_pss[4] * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LossSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossSHAP_Scores(df, hss):\n",
    "    \n",
    "    Weights = [0.2, 0.2, 0.2, 0.2, 0.2, \n",
    "               0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, \n",
    "               0.0333333,  0.0333333, 0.0333333, 0.0333333, 0.0333333, 0.0333333, 0.0333333, 0.0333333, 0.0333333, 0.0333333, \n",
    "               0.05, 0.05, 0.05, 0.05, 0.05,\n",
    "               0.2] \n",
    "\n",
    "    G1_idx = [0, 5, 6, 7, 8, 15, 16, 17, 18, 19, 20, 25, 26, 27, 28, 30]\n",
    "    G1_weights = [Weights[index] for index in G1_idx]   \n",
    "    G1_Sub = [(df.iloc[0]['HSS_Mean'] - hss), \n",
    "              (df.iloc[5]['HSS_Mean'] - df.iloc[1]['HSS_Mean']), \n",
    "              (df.iloc[6]['HSS_Mean'] - df.iloc[2]['HSS_Mean']), \n",
    "              (df.iloc[7]['HSS_Mean'] - df.iloc[3]['HSS_Mean']), \n",
    "              (df.iloc[8]['HSS_Mean'] - df.iloc[4]['HSS_Mean']), \n",
    "              (df.iloc[15]['HSS_Mean'] - df.iloc[9]['HSS_Mean']),  \n",
    "              (df.iloc[16]['HSS_Mean'] - df.iloc[10]['HSS_Mean']), \n",
    "              (df.iloc[17]['HSS_Mean'] - df.iloc[11]['HSS_Mean']), \n",
    "              (df.iloc[18]['HSS_Mean'] - df.iloc[12]['HSS_Mean']), \n",
    "              (df.iloc[19]['HSS_Mean'] - df.iloc[13]['HSS_Mean']), \n",
    "              (df.iloc[20]['HSS_Mean'] - df.iloc[14]['HSS_Mean']), \n",
    "              (df.iloc[25]['HSS_Mean'] - df.iloc[21]['HSS_Mean']),\n",
    "              (df.iloc[26]['HSS_Mean'] - df.iloc[22]['HSS_Mean']), \n",
    "              (df.iloc[27]['HSS_Mean'] - df.iloc[23]['HSS_Mean']), \n",
    "              (df.iloc[28]['HSS_Mean'] - df.iloc[24]['HSS_Mean']), \n",
    "              (hss - df.iloc[29]['HSS_Mean'])]\n",
    "    G1_score = sum([a*b for a, b in zip(G1_weights, G1_Sub)])\n",
    "    \n",
    "    G2_idx = [1, 5, 9, 10, 11, 15, 16, 17, 21, 22, 23, 25, 26, 27, 29, 30]\n",
    "    G2_weights = [Weights[index] for index in G2_idx]\n",
    "    G2_Sub = [(df.iloc[1]['HSS_Mean'] - hss), \n",
    "              (df.iloc[5]['HSS_Mean'] - df.iloc[0]['HSS_Mean']), \n",
    "              (df.iloc[9]['HSS_Mean'] - df.iloc[2]['HSS_Mean']), \n",
    "              (df.iloc[10]['HSS_Mean'] - df.iloc[3]['HSS_Mean']), \n",
    "              (df.iloc[11]['HSS_Mean'] - df.iloc[4]['HSS_Mean']), \n",
    "              (df.iloc[15]['HSS_Mean'] - df.iloc[6]['HSS_Mean']),  \n",
    "              (df.iloc[16]['HSS_Mean'] - df.iloc[7]['HSS_Mean']), \n",
    "              (df.iloc[17]['HSS_Mean'] - df.iloc[8]['HSS_Mean']), \n",
    "              (df.iloc[21]['HSS_Mean'] - df.iloc[12]['HSS_Mean']), \n",
    "              (df.iloc[22]['HSS_Mean'] - df.iloc[13]['HSS_Mean']), \n",
    "              (df.iloc[23]['HSS_Mean'] - df.iloc[14]['HSS_Mean']), \n",
    "              (df.iloc[25]['HSS_Mean'] - df.iloc[18]['HSS_Mean']),\n",
    "              (df.iloc[26]['HSS_Mean'] - df.iloc[19]['HSS_Mean']), \n",
    "              (df.iloc[27]['HSS_Mean'] - df.iloc[20]['HSS_Mean']), \n",
    "              (df.iloc[29]['HSS_Mean'] - df.iloc[24]['HSS_Mean']), \n",
    "              (hss - df.iloc[28]['HSS_Mean'])]\n",
    "    G2_score = sum([a*b for a, b in zip(G2_weights, G2_Sub)])\n",
    " \n",
    "    G3_idx = [2, 6, 9, 12, 13, 15, 18, 19, 21, 22, 24, 25, 26, 28, 29, 30]\n",
    "    G3_weights = [Weights[index] for index in G3_idx]\n",
    "    G3_Sub = [(df.iloc[2]['HSS_Mean'] - hss), \n",
    "              (df.iloc[6]['HSS_Mean'] - df.iloc[0]['HSS_Mean']), \n",
    "              (df.iloc[9]['HSS_Mean'] - df.iloc[1]['HSS_Mean']), \n",
    "              (df.iloc[12]['HSS_Mean'] - df.iloc[3]['HSS_Mean']), \n",
    "              (df.iloc[13]['HSS_Mean'] - df.iloc[4]['HSS_Mean']), \n",
    "              (df.iloc[15]['HSS_Mean'] - df.iloc[5]['HSS_Mean']),  \n",
    "              (df.iloc[18]['HSS_Mean'] - df.iloc[7]['HSS_Mean']), \n",
    "              (df.iloc[19]['HSS_Mean'] - df.iloc[8]['HSS_Mean']), \n",
    "              (df.iloc[21]['HSS_Mean'] - df.iloc[10]['HSS_Mean']), \n",
    "              (df.iloc[22]['HSS_Mean'] - df.iloc[11]['HSS_Mean']), \n",
    "              (df.iloc[24]['HSS_Mean'] - df.iloc[14]['HSS_Mean']), \n",
    "              (df.iloc[25]['HSS_Mean'] - df.iloc[16]['HSS_Mean']),\n",
    "              (df.iloc[26]['HSS_Mean'] - df.iloc[17]['HSS_Mean']), \n",
    "              (df.iloc[28]['HSS_Mean'] - df.iloc[20]['HSS_Mean']), \n",
    "              (df.iloc[29]['HSS_Mean'] - df.iloc[23]['HSS_Mean']), \n",
    "              (hss - df.iloc[27]['HSS_Mean'])]\n",
    "    G3_score = sum([a*b for a, b in zip(G3_weights, G3_Sub)])\n",
    "\n",
    "    G4_idx = [3, 7, 10, 12, 14, 16, 18, 20, 21, 23, 24, 26, 27, 28, 29, 30]\n",
    "    G4_weights = [Weights[index] for index in G4_idx]\n",
    "    G4_Sub = [(df.iloc[3]['HSS_Mean'] - hss), \n",
    "              (df.iloc[7]['HSS_Mean'] - df.iloc[0]['HSS_Mean']), \n",
    "              (df.iloc[10]['HSS_Mean'] - df.iloc[1]['HSS_Mean']), \n",
    "              (df.iloc[12]['HSS_Mean'] - df.iloc[2]['HSS_Mean']), \n",
    "              (df.iloc[14]['HSS_Mean'] - df.iloc[4]['HSS_Mean']), \n",
    "              (df.iloc[16]['HSS_Mean'] - df.iloc[5]['HSS_Mean']),  \n",
    "              (df.iloc[18]['HSS_Mean'] - df.iloc[6]['HSS_Mean']), \n",
    "              (df.iloc[20]['HSS_Mean'] - df.iloc[8]['HSS_Mean']), \n",
    "              (df.iloc[21]['HSS_Mean'] - df.iloc[9]['HSS_Mean']), \n",
    "              (df.iloc[23]['HSS_Mean'] - df.iloc[11]['HSS_Mean']), \n",
    "              (df.iloc[24]['HSS_Mean'] - df.iloc[13]['HSS_Mean']), \n",
    "              (df.iloc[25]['HSS_Mean'] - df.iloc[15]['HSS_Mean']),\n",
    "              (df.iloc[27]['HSS_Mean'] - df.iloc[17]['HSS_Mean']), \n",
    "              (df.iloc[28]['HSS_Mean'] - df.iloc[19]['HSS_Mean']), \n",
    "              (df.iloc[29]['HSS_Mean'] - df.iloc[22]['HSS_Mean']), \n",
    "              (hss - df.iloc[26]['HSS_Mean'])]\n",
    "    G4_score = sum([a*b for a, b in zip(G4_weights, G4_Sub)])\n",
    "\n",
    "    G5_idx     = [4, 8, 11, 13, 14, 17, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30] \n",
    "    G5_weights = [Weights[index] for index in G5_idx]\n",
    "    G5_Sub = [(df.iloc[4]['HSS_Mean'] - hss), \n",
    "              (df.iloc[8]['HSS_Mean'] - df.iloc[0]['HSS_Mean']), \n",
    "              (df.iloc[11]['HSS_Mean'] - df.iloc[1]['HSS_Mean']), \n",
    "              (df.iloc[13]['HSS_Mean'] - df.iloc[2]['HSS_Mean']), \n",
    "              (df.iloc[14]['HSS_Mean'] - df.iloc[3]['HSS_Mean']), \n",
    "              (df.iloc[17]['HSS_Mean'] - df.iloc[5]['HSS_Mean']),  \n",
    "              (df.iloc[19]['HSS_Mean'] - df.iloc[6]['HSS_Mean']), \n",
    "              (df.iloc[20]['HSS_Mean'] - df.iloc[7]['HSS_Mean']), \n",
    "              (df.iloc[22]['HSS_Mean'] - df.iloc[9]['HSS_Mean']), \n",
    "              (df.iloc[23]['HSS_Mean'] - df.iloc[10]['HSS_Mean']), \n",
    "              (df.iloc[24]['HSS_Mean'] - df.iloc[12]['HSS_Mean']), \n",
    "              (df.iloc[26]['HSS_Mean'] - df.iloc[15]['HSS_Mean']),\n",
    "              (df.iloc[27]['HSS_Mean'] - df.iloc[16]['HSS_Mean']), \n",
    "              (df.iloc[28]['HSS_Mean'] - df.iloc[18]['HSS_Mean']), \n",
    "              (df.iloc[29]['HSS_Mean'] - df.iloc[21]['HSS_Mean']), \n",
    "              (hss - df.iloc[25]['HSS_Mean'])]\n",
    "\n",
    "    G5_score = sum([a*b for a, b in zip(G5_weights, G5_Sub)])\n",
    "    \n",
    "    \n",
    "    return G1_score, G2_score, G3_score, G4_score, G5_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "LossSHAP_df = pd.read_csv('./LossSHAP.csv')\n",
    "\n",
    "G1_score, G2_score, G3_score, G4_score, G5_score = LossSHAP_Scores(LossSHAP_df, 0.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LossSHAP HSS based results:\n",
      "------ Score\n",
      "G1:    -9.39\n",
      "G2:    -4.19\n",
      "G3:    -6.33\n",
      "G4:    14.16\n",
      "G5:    5.75\n"
     ]
    }
   ],
   "source": [
    "print(\"LossSHAP HSS based results:\")\n",
    "print(\"------ Score\")\n",
    "print(f\"G1:    {G1_score * 100:.2f}\")\n",
    "print(f\"G2:    {G2_score * 100:.2f}\")\n",
    "print(f\"G3:    {G3_score * 100:.2f}\")\n",
    "print(f\"G4:    {G4_score * 100:.2f}\")\n",
    "print(f\"G5:    {G5_score * 100:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('deep-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7 (default, Mar 26 2020, 15:48:22) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e69a14dfbdec224c5e932b370c8d5491158516262a04bb50e9804f010a1e34c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
