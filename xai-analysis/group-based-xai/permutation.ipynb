{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Requirements: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import os\n",
    "import os.path \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import copy\n",
    "import errno\n",
    "import glob\n",
    "import time\n",
    "import calendar\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import netCDF4\n",
    "from numpy import savez_compressed\n",
    "from optparse import OptionParser\n",
    "from scipy.interpolate import (UnivariateSpline, RectBivariateSpline, RegularGridInterpolator)\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model   \n",
    "from tensorflow.keras.layers import Add, add, concatenate, Reshape, BatchNormalization, Input, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, Conv3D, Activation, MaxPooling2D, MaxPooling3D, AveragePooling3D, ReLU, GlobalAveragePooling3D, multiply\n",
    "\n",
    "from tensorflow.keras import regularizers \n",
    "from tensorflow.keras import optimizers \n",
    "from tensorflow.keras.optimizers import Adam, SGD \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import FogNet\n",
    "import FogNetConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend.tensorflow_backend as tfback\n",
    "#print(\"tf.version is\", tf.version)\n",
    "#print(\"tf.keras.version is:\", tf.keras.version)\n",
    "\n",
    "def _get_available_gpus():\n",
    "\tif tfback._LOCAL_DEVICES is None:\n",
    "\t    devices = tensorflow.config.list_logical_devices()\n",
    "\t    tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
    "\treturn [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
    "tfback._get_available_gpus = _get_available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_IMAGE_DIR_NAME = ('/data1/fog-data/fog-maps/')\n",
    "DEFAULT_TARGET_DIR_NAME = ('/data1/fog/fognn/Dataset/TARGET/')\n",
    "\n",
    "DEFAULT_CUBES_24_DIR_NAME = ('/data1/fog/fognn/Dataset/24HOURS/INPUT/')\n",
    "DEFAULT_TARGET_24_DIR_NAME = ('/data1/fog/fognn/Dataset/24HOURS/TARGET/')\n",
    "\n",
    "DEFAULT_LINE_COLOUR = numpy.array([228, 26, 28], dtype=float) / 255\n",
    "DEFAULT_LINE_WIDTH = 3\n",
    "DEFAULT_RANDOM_LINE_COLOUR = numpy.full(3, 152. / 255)\n",
    "DEFAULT_RANDOM_LINE_WIDTH = 2\n",
    "\n",
    "LEVELS_FOR_CONTOURS = numpy.linspace(0, 1, num=11, dtype=float)\n",
    "\n",
    "FIGURE_WIDTH_INCHES = 10\n",
    "FIGURE_HEIGHT_INCHES = 10\n",
    "\n",
    "FONT_SIZE = 20\n",
    "plt.rc('font', size=FONT_SIZE)\n",
    "plt.rc('axes', titlesize=FONT_SIZE)\n",
    "plt.rc('axes', labelsize=FONT_SIZE)\n",
    "plt.rc('xtick', labelsize=FONT_SIZE)\n",
    "plt.rc('ytick', labelsize=FONT_SIZE)\n",
    "plt.rc('legend', fontsize=FONT_SIZE)\n",
    "plt.rc('figure', titlesize=FONT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained FogNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Setup input data rasters #\n",
    "############################\n",
    "# Generate data file paths\n",
    "trainYearIdxs = [4, 5, 6, 7, 8]\n",
    "valYearIdxs   = [0, 1, 2, 3]\n",
    "testYearIdxs  = [9, 10, 11]\n",
    "\n",
    "horizons = [6, 12, 24]\n",
    "allYears = [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "\n",
    "nam_G1_template = \"NETCDF_NAM_CUBE_{year}_PhG1_{horizon}.npz\"\n",
    "nam_G1_names = [nam_G1_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "nam_G2_template = \"NETCDF_NAM_CUBE_{year}_PhG2_{horizon}.npz\"\n",
    "nam_G2_names = [nam_G2_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "nam_G3_template = \"NETCDF_NAM_CUBE_{year}_PhG3_{horizon}.npz\"\n",
    "nam_G3_names = [nam_G3_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "nam_G4_template = \"NETCDF_NAM_CUBE_{year}_PhG4_{horizon}.npz\"\n",
    "nam_G4_names = [nam_G4_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "mixed_file_template = \"NETCDF_MIXED_CUBE_{year}_{horizon}.npz\"\n",
    "mixed_file_names = [mixed_file_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "mur_file_template = \"NETCDF_SST_CUBE_{year}.npz\"\n",
    "mur_file_names = [mur_file_template.format(year=year) for year in allYears]\n",
    "\n",
    "targets_file_template = \"target{year}_{horizon}.csv\"\n",
    "targets_file_names = [targets_file_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "strategy = tensorflow.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "with strategy.scope(): \n",
    "# Read data cubes\n",
    "    training_list   = utils.load_Cat_cube_data(nam_G1_names,\n",
    "        nam_G2_names, nam_G3_names, nam_G4_names, mixed_file_names, mur_file_names, DEFAULT_CUBES_24_DIR_NAME, trainYearIdxs)\n",
    "    validation_list = utils.load_Cat_cube_data(nam_G1_names,\n",
    "        nam_G2_names, nam_G3_names, nam_G4_names, mixed_file_names, mur_file_names, DEFAULT_CUBES_24_DIR_NAME, valYearIdxs)\n",
    "    testing_list    = utils.load_Cat_cube_data(nam_G1_names,\n",
    "        nam_G2_names, nam_G3_names, nam_G4_names, mixed_file_names, mur_file_names, DEFAULT_CUBES_24_DIR_NAME, testYearIdxs)\n",
    "\n",
    "    target_class = utils.targets(\n",
    "        targets_file_names, trainYearIdxs, valYearIdxs, testYearIdxs,\n",
    "        DEFAULT_TARGET_24_DIR_NAME,\n",
    "        0, # priority_calss: the last integer value is the class of target to predict: 0: is < 1600; 1: < 3200 and 2: < 6400\n",
    "    )\n",
    "    target_list = target_class.binary_target()\n",
    "\n",
    "    # Separate into train, test, validation\n",
    "    Training_targets = target_list[0]\n",
    "    print('training target shape:', Training_targets.shape)\n",
    "    ytrain = target_list[1]\n",
    "    print('training categorical target shape:', ytrain.shape)\n",
    "    Validation_targets = target_list[2]\n",
    "    print('validation target shape:', Validation_targets.shape)\n",
    "    yvalid = target_list[3]\n",
    "    print('validation categorical target shape:', yvalid.shape)\n",
    "    Testing_targets = target_list[4]\n",
    "    print('testing target shape:', Testing_targets.shape)\n",
    "    ytest = target_list[5]\n",
    "    print('testing categorical target shape:', ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "learningRate = 0.0009 # hyperparameters[key][0] \n",
    "wd           = 0.001  # hyperparameters[key][1] \n",
    "filters      = 24     # hyperparameters[key][2] \n",
    "dropout      = 0.3    # hyperparameters[key][3] \n",
    "cnn_file_name = '/data1/fog/fognn/FogNet/trained_model/single_gpu_weights.h5'\n",
    "\n",
    "C  = FogNet.FogNet(\n",
    "    Input(training_list[0].shape[1:]),\n",
    "    Input(training_list[1].shape[1:]),\n",
    "    Input(training_list[2].shape[1:]),\n",
    "    Input(training_list[3].shape[1:]),\n",
    "    Input(training_list[4].shape[1:]),\n",
    "    Input(training_list[5].shape[1:]),\n",
    "    filters, dropout, 2)\n",
    "cnn_model_object = C.BuildModel()\n",
    "#model.summary() \n",
    "\n",
    "cnn_model_object.load_weights(cnn_file_name)  \n",
    "\n",
    "cnn_model_object.compile(optimizer=Adam(lr=learningRate, decay=wd),\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import confusion_matrix\n",
    "def test_eval(y, ypred, th = None): \n",
    "    length = len(ypred) \n",
    "    ypred_ = [0]*length\n",
    "\n",
    "    for i in range(length):\n",
    "        prob = ypred[i, 1] \n",
    "        if prob > th:\n",
    "            ypred_[i] = 1\n",
    "        else:\n",
    "            ypred_[i] = 0\n",
    "    ypred_ = numpy.array(ypred_)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, ypred_).ravel()\n",
    "    a = tn     # Hit\n",
    "    b = fn      # false alarm\n",
    "    c = fp      # miss\n",
    "    d = tp    # correct rejection \n",
    "\n",
    "    POD = a/(a+c)\n",
    "    F   = b/(b+d)\n",
    "    FAR  = b/(a+b)\n",
    "    CSI = a/(a+b+c)\n",
    "    PSS = ((a*d)-(b*c))/((b+d)*(a+c))\n",
    "    HSS = (2*((a*d)-(b*c)))/(((a+c)*(c+d))+((a+b)*(b+d)))\n",
    "    ORSS = ((a*d)-(b*c))/((a*d)+(b*c))\n",
    "    CSS = ((a*d)-(b*c))/((a+b)*(c+d))\n",
    "\n",
    "    #print('POD  : ', POD) \n",
    "    #print('F    : ', F)\n",
    "    #print('FAR  : ', FAR)\n",
    "    #print('CSI  : ', CSI)\n",
    "    #print('PSS  : ', PSS)\n",
    "    #print('HSS  : ', HSS)\n",
    "    #print('ORSS : ', ORSS)\n",
    "    #print('CSS  : ', CSS)\n",
    "    return [POD, F, FAR, CSI, PSS, HSS, ORSS, CSS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = numpy.ones(shape=(2228,32,32,108,1))\n",
    "import copy\n",
    "list_cpy = copy.deepcopy(testing_list)\n",
    "\n",
    "\n",
    "permuted_map = numpy.random.permutation(list_cpy[1][:,:,:,5,:]) \n",
    "list_cpy[1][:,:,:,5,:] = permuted_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_list[1][:,:,:,5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = numpy.array([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]])\n",
    "\n",
    "for f in range(4):\n",
    "    for i in range(1): \n",
    "        input_data_copy = numpy.copy(a)\n",
    "        print(a)\n",
    "        numpy.random.seed(42)\n",
    "        permuted_map = numpy.random.permutation(input_data_copy[f,:]) \n",
    "        input_data_copy[f,:] = permuted_map\n",
    "        print(input_data_copy)\n",
    "        print('/n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importance_(model_object, input_data, input_target, n_repeats=None, random_state=42): \n",
    "    \n",
    "    df = pd.DataFrame(columns = ['Feature', 'POD_mean', 'POD_std', 'F_mean', 'F_std','FAR_mean', 'FAR_std','CSI_mean', 'CSI_std',\n",
    "                                 'PSS_mean', 'PSS_std', 'HSS_mean', 'HSS_std', 'ORSS_mean', 'ORSS_std', 'CSS_mean', 'CSS_std'])\n",
    "    \n",
    "    this_pod, this_f, this_far, this_csi, this_pss, this_hss, this_orss, this_css = [],[],[],[],[],[],[],[]\n",
    "    fnames = []\n",
    "    n_groups   = len(input_data)\n",
    "    #for g in range(n_groups): \n",
    "    g = 3\n",
    "    if g ==0:\n",
    "        GNames = utils.NETCDF_PREDICTOR_NAMES['Physical_G1']\n",
    "    if g ==1:\n",
    "        GNames = utils.NETCDF_PREDICTOR_NAMES['Physical_G2']\n",
    "    if g ==2:\n",
    "        GNames = utils.NETCDF_PREDICTOR_NAMES['Physical_G3']\n",
    "    if g ==3:\n",
    "        GNames = utils.NETCDF_PREDICTOR_NAMES['Physical_G4']\n",
    "    if g ==4:\n",
    "        GNames = utils.NETCDF_PREDICTOR_NAMES['Mixed']\n",
    "    if g ==5:\n",
    "        GNames = utils.NETCDF_PREDICTOR_NAMES['SST']        \n",
    "\n",
    "    n_features = input_data[g].shape[3]\n",
    "# orig=inut_data[g],  then innput_data[g] = permuted.  Then test, then set input_data[g] = orig\n",
    "\n",
    "\n",
    "    for f in range(n_features):\n",
    "        for i in range(n_repeats): \n",
    "            input_data_copy = np.copy(input_data)\n",
    "            \n",
    "            numpy.random.seed(random_state)\n",
    "            permuted_map = numpy.random.permutation(input_data_copy[g][:,:,:,f,:]) \n",
    "            permuted_data  = input_data_copy[g]\n",
    "            input_data_copy[g][:,:,:,f,:] = permuted_map\n",
    "\n",
    "            y_testing_cat_prob = model_object.predict(input_data_copy) \n",
    "            metric_list = test_eval(input_target, y_testing_cat_prob, th = 0.193)\n",
    "\n",
    "            this_pod.append(metric_list[0])\n",
    "            this_f.append(metric_list[1])\n",
    "            this_far.append(metric_list[2])\n",
    "            this_csi.append(metric_list[3])\n",
    "            this_pss.append(metric_list[4])\n",
    "            this_hss.append(metric_list[5])\n",
    "            this_orss.append(metric_list[6])\n",
    "            this_css.append(metric_list[7])\n",
    "\n",
    "            feature_name     = GNames[int(numpy.floor(f/4))]\n",
    "            fnames.append(feature_name)\n",
    "            print(f\"{feature_name}: HSS = {numpy.mean(this_hss)}/{numpy.std(this_hss)}\") \n",
    "            \n",
    "            \n",
    "#print(f\"The calculation for feature {feature_name} is done!\")\n",
    "    df['Feature']    = fnames\n",
    "    df['POD_mean']   = numpy.mean(this_pod)\n",
    "    df['POD_std']    = numpy.std(this_pod)\n",
    "    df['F_mean']     = numpy.mean(this_f)\n",
    "    df['F_std']      = numpy.std(this_f)\n",
    "    df['FAR_mean']   = numpy.mean(this_far)\n",
    "    df['FAR_std']    = numpy.std(this_far)\n",
    "    df['CSI_mean']   = numpy.mean(this_csi)\n",
    "    df['CSI_std']    = numpy.std(this_csi)\n",
    "    df['PSS_mean']   = numpy.mean(this_pss)\n",
    "    df['PSS_std']    = numpy.std(this_pss)\n",
    "    df['HSS_mean']   = numpy.mean(this_hss)\n",
    "    df['HSS_std']    = numpy.std(this_hss)\n",
    "    df['ORSS_mean']  = numpy.mean(this_orss)\n",
    "    df['ORSS_std']   = numpy.std(this_orss)\n",
    "    df['CSS_mean']   = numpy.mean(this_css)\n",
    "    df['CSS_std']    = numpy.std(this_css)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Permutation_R = permutation_importance_(cnn_model_object, testing_list, Testing_targets, n_repeats=1, random_state=42)\n",
    "Permutation_R.to_csv('./g3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visulization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permut = pd.read_csv('./NewR.csv')\n",
    "print(len(permut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "\n",
    "new_df = pd.DataFrame(columns = ['feature', 'hss-mean', 'hss_std'])\n",
    "\n",
    "f, m, s = [],[],[]\n",
    "\n",
    "for i in range(len(permut)):\n",
    "    f_name = permut.loc[t]['Feature']\n",
    "    #print(f_name)\n",
    "    mean_hss = permut.loc[t]['HSS_mean']\n",
    "    std_hss  = permut.loc[t]['HSS_std']\n",
    "    \n",
    "    f.append(f_name)\n",
    "    m.append(mean_hss)\n",
    "    s.append(std_hss)\n",
    "    #leadtime = \n",
    "\n",
    "    t = t+2\n",
    "    \n",
    "new_df['feature']  = f  \n",
    "new_df['hss-mean'] = m\n",
    "new_df['hss_std']  = s\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g1 = ['g1']*108\n",
    "g2 = ['g2']*96\n",
    "g3 = ['g3']*108\n",
    "g4 = ['g4']*60\n",
    "g5 = ['g5']*12\n",
    "group_id = g1+g2+g3+g4+g5\n",
    "#group_id = [g1, g2, g3, g4, g5]\n",
    "len(group_id)\n",
    "\n",
    "permut['Score'] = numpy.abs((permut['HSS_mean']*100) - 52)\n",
    "permut = permut.drop(index = 384)\n",
    "permut['groups'] = group_id\n",
    "permut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permut.to_csv('./NewR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "permut = pd.read_csv()\n",
    "\n",
    "fig, axs = plt.subplots(figsize = (20, 8))\n",
    "ax = sns.barplot(x=\"Feature\", y=\"Score\", data=permut, hue= 'groups', ax = axs)\n",
    "\n",
    "plt.xticks(rotation=90, fontsize = 12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
