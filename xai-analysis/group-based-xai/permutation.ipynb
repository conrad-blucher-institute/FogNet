{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Requirements: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import os\n",
    "import os.path \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import copy\n",
    "import errno\n",
    "import glob\n",
    "import time\n",
    "import calendar\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import netCDF4\n",
    "from numpy import savez_compressed\n",
    "from optparse import OptionParser\n",
    "from scipy.interpolate import (UnivariateSpline, RectBivariateSpline, RegularGridInterpolator)\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model   \n",
    "from tensorflow.keras.layers import Add, add, concatenate, Reshape, BatchNormalization, Input, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, Conv3D, Activation, MaxPooling2D, MaxPooling3D, AveragePooling3D, ReLU, GlobalAveragePooling3D, multiply\n",
    "\n",
    "from tensorflow.keras import regularizers \n",
    "from tensorflow.keras import optimizers \n",
    "from tensorflow.keras.optimizers import Adam, SGD \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils, FogNet, FogNetConfig, cnn_evaluate, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend.tensorflow_backend as tfback\n",
    "#print(\"tf.version is\", tf.version)\n",
    "#print(\"tf.keras.version is:\", tf.keras.version)\n",
    "\n",
    "def _get_available_gpus():\n",
    "\tif tfback._LOCAL_DEVICES is None:\n",
    "\t    devices = tensorflow.config.list_logical_devices()\n",
    "\t    tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
    "\treturn [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
    "tfback._get_available_gpus = _get_available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_IMAGE_DIR_NAME = ('/data1/fog-data/fog-maps/')\n",
    "DEFAULT_TARGET_DIR_NAME = ('/data1/fog/fognn/Dataset/TARGET/')\n",
    "\n",
    "DEFAULT_CUBES_24_DIR_NAME = ('/data1/fog/fognn/Dataset/24HOURS/INPUT/')\n",
    "DEFAULT_TARGET_24_DIR_NAME = ('/data1/fog/fognn/Dataset/24HOURS/TARGET/')\n",
    "\n",
    "DEFAULT_LINE_COLOUR = numpy.array([228, 26, 28], dtype=float) / 255\n",
    "DEFAULT_LINE_WIDTH = 3\n",
    "DEFAULT_RANDOM_LINE_COLOUR = numpy.full(3, 152. / 255)\n",
    "DEFAULT_RANDOM_LINE_WIDTH = 2\n",
    "\n",
    "LEVELS_FOR_CONTOURS = numpy.linspace(0, 1, num=11, dtype=float)\n",
    "\n",
    "FIGURE_WIDTH_INCHES = 10\n",
    "FIGURE_HEIGHT_INCHES = 10\n",
    "\n",
    "FONT_SIZE = 20\n",
    "plt.rc('font', size=FONT_SIZE)\n",
    "plt.rc('axes', titlesize=FONT_SIZE)\n",
    "plt.rc('axes', labelsize=FONT_SIZE)\n",
    "plt.rc('xtick', labelsize=FONT_SIZE)\n",
    "plt.rc('ytick', labelsize=FONT_SIZE)\n",
    "plt.rc('legend', fontsize=FONT_SIZE)\n",
    "plt.rc('figure', titlesize=FONT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained FogNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-cc9a4901e2ab>:34: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3')\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3'), communication = CommunicationImplementation.AUTO\n",
      "Number of devices: 4\n",
      "NAM_G1 input size:  (5460, 32, 32, 108, 1)\n",
      "NAM_G2 input size:  (5460, 32, 32, 96, 1)\n",
      "NAM_G3 input size:  (5460, 32, 32, 108, 1)\n",
      "NAM_G4 input size:  (5460, 32, 32, 60, 1)\n",
      "MIXED input size:  (5460, 32, 32, 12, 1)\n",
      "MUR input size:  (5460, 384, 384, 1, 1)\n",
      "NAM_G1 input size:  (3328, 32, 32, 108, 1)\n",
      "NAM_G2 input size:  (3328, 32, 32, 96, 1)\n",
      "NAM_G3 input size:  (3328, 32, 32, 108, 1)\n",
      "NAM_G4 input size:  (3328, 32, 32, 60, 1)\n",
      "MIXED input size:  (3328, 32, 32, 12, 1)\n",
      "MUR input size:  (3328, 384, 384, 1, 1)\n",
      "NAM_G1 input size:  (2228, 32, 32, 108, 1)\n",
      "NAM_G2 input size:  (2228, 32, 32, 96, 1)\n",
      "NAM_G3 input size:  (2228, 32, 32, 108, 1)\n",
      "NAM_G4 input size:  (2228, 32, 32, 60, 1)\n",
      "MIXED input size:  (2228, 32, 32, 12, 1)\n",
      "MUR input size:  (2228, 384, 384, 1, 1)\n",
      "training target shape: (5460,)\n",
      "training categorical target shape: (5460, 2)\n",
      "validation target shape: (3328,)\n",
      "validation categorical target shape: (3328, 2)\n",
      "testing target shape: (2228,)\n",
      "testing categorical target shape: (2228, 2)\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Setup input data rasters #\n",
    "############################\n",
    "# Generate data file paths\n",
    "trainYearIdxs = [4, 5, 6, 7, 8]\n",
    "valYearIdxs   = [0, 1, 2, 3]\n",
    "testYearIdxs  = [9, 10, 11]\n",
    "\n",
    "horizons = [6, 12, 24]\n",
    "allYears = [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "\n",
    "nam_G1_template = \"NETCDF_NAM_CUBE_{year}_PhG1_{horizon}.npz\"\n",
    "nam_G1_names = [nam_G1_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "nam_G2_template = \"NETCDF_NAM_CUBE_{year}_PhG2_{horizon}.npz\"\n",
    "nam_G2_names = [nam_G2_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "nam_G3_template = \"NETCDF_NAM_CUBE_{year}_PhG3_{horizon}.npz\"\n",
    "nam_G3_names = [nam_G3_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "nam_G4_template = \"NETCDF_NAM_CUBE_{year}_PhG4_{horizon}.npz\"\n",
    "nam_G4_names = [nam_G4_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "mixed_file_template = \"NETCDF_MIXED_CUBE_{year}_{horizon}.npz\"\n",
    "mixed_file_names = [mixed_file_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "mur_file_template = \"NETCDF_SST_CUBE_{year}.npz\"\n",
    "mur_file_names = [mur_file_template.format(year=year) for year in allYears]\n",
    "\n",
    "targets_file_template = \"target{year}_{horizon}.csv\"\n",
    "targets_file_names = [targets_file_template.format(year=year, horizon=horizons[2]) for year in allYears]\n",
    "\n",
    "strategy = tensorflow.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "with strategy.scope(): \n",
    "# Read data cubes\n",
    "    training_list   = utils.load_Cat_cube_data(nam_G1_names,\n",
    "        nam_G2_names, nam_G3_names, nam_G4_names, mixed_file_names, mur_file_names, DEFAULT_CUBES_24_DIR_NAME, trainYearIdxs)\n",
    "    validation_list = utils.load_Cat_cube_data(nam_G1_names,\n",
    "        nam_G2_names, nam_G3_names, nam_G4_names, mixed_file_names, mur_file_names, DEFAULT_CUBES_24_DIR_NAME, valYearIdxs)\n",
    "    testing_list    = utils.load_Cat_cube_data(nam_G1_names,\n",
    "        nam_G2_names, nam_G3_names, nam_G4_names, mixed_file_names, mur_file_names, DEFAULT_CUBES_24_DIR_NAME, testYearIdxs)\n",
    "\n",
    "    target_class = utils.targets(\n",
    "        targets_file_names, trainYearIdxs, valYearIdxs, testYearIdxs,\n",
    "        DEFAULT_TARGET_24_DIR_NAME,\n",
    "        0, # priority_calss: the last integer value is the class of target to predict: 0: is < 1600; 1: < 3200 and 2: < 6400\n",
    "    )\n",
    "    target_list = target_class.binary_target()\n",
    "\n",
    "    # Separate into train, test, validation\n",
    "    Training_targets = target_list[0]\n",
    "    print('training target shape:', Training_targets.shape)\n",
    "    ytrain = target_list[1]\n",
    "    print('training categorical target shape:', ytrain.shape)\n",
    "    Validation_targets = target_list[2]\n",
    "    print('validation target shape:', Validation_targets.shape)\n",
    "    yvalid = target_list[3]\n",
    "    print('validation categorical target shape:', yvalid.shape)\n",
    "    Testing_targets = target_list[4]\n",
    "    print('testing target shape:', Testing_targets.shape)\n",
    "    ytest = target_list[5]\n",
    "    print('testing categorical target shape:', ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multi_gpu_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m C  \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mFogNet(input_nam_G1_24_shape, \n\u001b[1;32m     17\u001b[0m                    input_nam_G2_24_shape, \n\u001b[1;32m     18\u001b[0m                    input_nam_G3_24_shape, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m                    input_mur_shape,\n\u001b[1;32m     22\u001b[0m                    filters, dropout, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     23\u001b[0m cnn_model_object \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mBuildModel()\n\u001b[0;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_gpu_model\u001b[49m(model, gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#model.summary() \u001b[39;00m\n\u001b[1;32m     27\u001b[0m cnn_model_object\u001b[38;5;241m.\u001b[39mload_weights(cnn_file_name)  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'multi_gpu_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "strategy = tensorflow.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "with strategy.scope():\n",
    "    input_nam_shape       = Input((32, 32, 288, 1))\n",
    "    input_mur_shape       = Input((384, 384, 1, 1)) \n",
    "    input_nam_G1_24_shape = Input((32, 32, 108, 1))\n",
    "    input_nam_G2_24_shape = Input((32, 32, 96, 1))\n",
    "    input_nam_G3_24_shape = Input((32, 32, 108, 1))\n",
    "    input_nam_G4_24_shape = Input((32, 32, 60, 1))\n",
    "    input_mixed_24_shape  = Input((32, 32, 12, 1))\n",
    "\n",
    "    learningRate = 0.0009 # hyperparameters[key][0] \n",
    "    wd           = 0.001  # hyperparameters[key][1] \n",
    "    filters      = 24     # hyperparameters[key][2] \n",
    "    dropout      = 0.3    # hyperparameters[key][3] \n",
    "    cnn_file_name = '/data1/fog/fognn/FogNet/trained_model/weights.h5'\n",
    "\n",
    "    C  = models.FogNet(input_nam_G1_24_shape, \n",
    "                       input_nam_G2_24_shape, \n",
    "                       input_nam_G3_24_shape, \n",
    "                       input_nam_G4_24_shape, \n",
    "                       input_mixed_24_shape, \n",
    "                       input_mur_shape,\n",
    "                       filters, dropout, 2)\n",
    "    cnn_model_object = C.BuildModel()\n",
    "\n",
    "    #model.summary() \n",
    "\n",
    "    cnn_model_object.load_weights(cnn_file_name)  \n",
    "\n",
    "    cnn_model_object.compile(optimizer=Adam(lr=learningRate, decay=wd),\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import confusion_matrix\n",
    "def test_eval(y, ypred, th = None): \n",
    "    length = len(ypred) \n",
    "    ypred_ = [0]*length\n",
    "\n",
    "    for i in range(length):\n",
    "        prob = ypred[i, 1] \n",
    "        if prob > th:\n",
    "            ypred_[i] = 1\n",
    "        else:\n",
    "            ypred_[i] = 0\n",
    "    ypred_ = numpy.array(ypred_)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, ypred_).ravel()\n",
    "    a = tn     # Hit\n",
    "    b = fn      # false alarm\n",
    "    c = fp      # miss\n",
    "    d = tp    # correct rejection \n",
    "\n",
    "    POD = a/(a+c)\n",
    "    F   = b/(b+d)\n",
    "    FAR  = b/(a+b)\n",
    "    CSI = a/(a+b+c)\n",
    "    PSS = ((a*d)-(b*c))/((b+d)*(a+c))\n",
    "    HSS = (2*((a*d)-(b*c)))/(((a+c)*(c+d))+((a+b)*(b+d)))\n",
    "    ORSS = ((a*d)-(b*c))/((a*d)+(b*c))\n",
    "    CSS = ((a*d)-(b*c))/((a+b)*(c+d))\n",
    "\n",
    "    #print('POD  : ', POD) \n",
    "    #print('F    : ', F)\n",
    "    #print('FAR  : ', FAR)\n",
    "    #print('CSI  : ', CSI)\n",
    "    #print('PSS  : ', PSS)\n",
    "    #print('HSS  : ', HSS)\n",
    "    #print('ORSS : ', ORSS)\n",
    "    #print('CSS  : ', CSS)\n",
    "    return [POD, F, FAR, CSI, PSS, HSS, ORSS, CSS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = numpy.ones(shape=(2228,32,32,108,1))\n",
    "import copy\n",
    "list_cpy = copy.deepcopy(testing_list)\n",
    "\n",
    "\n",
    "permuted_map = numpy.random.permutation(list_cpy[1][:,:,:,5,:]) \n",
    "list_cpy[1][:,:,:,5,:] = permuted_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_list[1][:,:,:,5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = numpy.array([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]])\n",
    "\n",
    "for f in range(4):\n",
    "    for i in range(1): \n",
    "        input_data_copy = numpy.copy(a)\n",
    "        print(a)\n",
    "        numpy.random.seed(42)\n",
    "        permuted_map = numpy.random.permutation(input_data_copy[f,:]) \n",
    "        input_data_copy[f,:] = permuted_map\n",
    "        print(input_data_copy)\n",
    "        print('/n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importance_(model_object, input_data, input_target, n_repeats=None, random_state=42): \n",
    "    \n",
    "    df = pd.DataFrame(columns = ['Feature', 'POD_mean', 'POD_std', 'F_mean', 'F_std','FAR_mean', 'FAR_std','CSI_mean', 'CSI_std',\n",
    "                                 'PSS_mean', 'PSS_std', 'HSS_mean', 'HSS_std', 'ORSS_mean', 'ORSS_std', 'CSS_mean', 'CSS_std'])\n",
    "    \n",
    "    this_pod, this_f, this_far, this_csi, this_pss, this_hss, this_orss, this_css = [],[],[],[],[],[],[],[]\n",
    "    fnames = []\n",
    "    n_groups   = len(input_data)\n",
    "    #for g in range(n_groups): \n",
    "    g = 3\n",
    "    if g ==0:\n",
    "        GNames = utils.NETCDF_PREDICTOR_NAMES['Physical_G1']\n",
    "    if g ==1:\n",
    "        GNames = utils.NETCDF_PREDICTOR_NAMES['Physical_G2']\n",
    "    if g ==2:\n",
    "        GNames = utils.NETCDF_PREDICTOR_NAMES['Physical_G3']\n",
    "    if g ==3:\n",
    "        GNames = utils.NETCDF_PREDICTOR_NAMES['Physical_G4']\n",
    "    if g ==4:\n",
    "        GNames = utils.NETCDF_PREDICTOR_NAMES['Mixed']\n",
    "    if g ==5:\n",
    "        GNames = utils.NETCDF_PREDICTOR_NAMES['SST']        \n",
    "\n",
    "    n_features = input_data[g].shape[3]\n",
    "# orig=inut_data[g],  then innput_data[g] = permuted.  Then test, then set input_data[g] = orig\n",
    "\n",
    "\n",
    "    for f in range(n_features):\n",
    "        for i in range(n_repeats): \n",
    "            input_data_copy = np.copy(input_data)\n",
    "            \n",
    "            numpy.random.seed(random_state)\n",
    "            permuted_map = numpy.random.permutation(input_data_copy[g][:,:,:,f,:]) \n",
    "            permuted_data  = input_data_copy[g]\n",
    "            input_data_copy[g][:,:,:,f,:] = permuted_map\n",
    "\n",
    "            y_testing_cat_prob = model_object.predict(input_data_copy) \n",
    "            metric_list = test_eval(input_target, y_testing_cat_prob, th = 0.193)\n",
    "\n",
    "            this_pod.append(metric_list[0])\n",
    "            this_f.append(metric_list[1])\n",
    "            this_far.append(metric_list[2])\n",
    "            this_csi.append(metric_list[3])\n",
    "            this_pss.append(metric_list[4])\n",
    "            this_hss.append(metric_list[5])\n",
    "            this_orss.append(metric_list[6])\n",
    "            this_css.append(metric_list[7])\n",
    "\n",
    "            feature_name     = GNames[int(numpy.floor(f/4))]\n",
    "            fnames.append(feature_name)\n",
    "            print(f\"{feature_name}: HSS = {numpy.mean(this_hss)}/{numpy.std(this_hss)}\") \n",
    "            \n",
    "            \n",
    "#print(f\"The calculation for feature {feature_name} is done!\")\n",
    "    df['Feature']    = fnames\n",
    "    df['POD_mean']   = numpy.mean(this_pod)\n",
    "    df['POD_std']    = numpy.std(this_pod)\n",
    "    df['F_mean']     = numpy.mean(this_f)\n",
    "    df['F_std']      = numpy.std(this_f)\n",
    "    df['FAR_mean']   = numpy.mean(this_far)\n",
    "    df['FAR_std']    = numpy.std(this_far)\n",
    "    df['CSI_mean']   = numpy.mean(this_csi)\n",
    "    df['CSI_std']    = numpy.std(this_csi)\n",
    "    df['PSS_mean']   = numpy.mean(this_pss)\n",
    "    df['PSS_std']    = numpy.std(this_pss)\n",
    "    df['HSS_mean']   = numpy.mean(this_hss)\n",
    "    df['HSS_std']    = numpy.std(this_hss)\n",
    "    df['ORSS_mean']  = numpy.mean(this_orss)\n",
    "    df['ORSS_std']   = numpy.std(this_orss)\n",
    "    df['CSS_mean']   = numpy.mean(this_css)\n",
    "    df['CSS_std']    = numpy.std(this_css)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Permutation_R = permutation_importance_(cnn_model_object, testing_list, Testing_targets, n_repeats=1, random_state=42)\n",
    "Permutation_R.to_csv('./g3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visulization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permut = pd.read_csv('./NewR.csv')\n",
    "print(len(permut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "\n",
    "new_df = pd.DataFrame(columns = ['feature', 'hss-mean', 'hss_std'])\n",
    "\n",
    "f, m, s = [],[],[]\n",
    "\n",
    "for i in range(len(permut)):\n",
    "    f_name = permut.loc[t]['Feature']\n",
    "    #print(f_name)\n",
    "    mean_hss = permut.loc[t]['HSS_mean']\n",
    "    std_hss  = permut.loc[t]['HSS_std']\n",
    "    \n",
    "    f.append(f_name)\n",
    "    m.append(mean_hss)\n",
    "    s.append(std_hss)\n",
    "    #leadtime = \n",
    "\n",
    "    t = t+2\n",
    "    \n",
    "new_df['feature']  = f  \n",
    "new_df['hss-mean'] = m\n",
    "new_df['hss_std']  = s\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g1 = ['g1']*108\n",
    "g2 = ['g2']*96\n",
    "g3 = ['g3']*108\n",
    "g4 = ['g4']*60\n",
    "g5 = ['g5']*12\n",
    "group_id = g1+g2+g3+g4+g5\n",
    "#group_id = [g1, g2, g3, g4, g5]\n",
    "len(group_id)\n",
    "\n",
    "permut['Score'] = numpy.abs((permut['HSS_mean']*100) - 52)\n",
    "permut = permut.drop(index = 384)\n",
    "permut['groups'] = group_id\n",
    "permut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permut.to_csv('./NewR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "permut = pd.read_csv()\n",
    "\n",
    "fig, axs = plt.subplots(figsize = (20, 8))\n",
    "ax = sns.barplot(x=\"Feature\", y=\"Score\", data=permut, hue= 'groups', ax = axs)\n",
    "\n",
    "plt.xticks(rotation=90, fontsize = 12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
